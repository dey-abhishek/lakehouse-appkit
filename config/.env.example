# ═══════════════════════════════════════════════════════════════════════════════
# Lakehouse-AppKit Development Configuration
# ═══════════════════════════════════════════════════════════════════════════════
#
# Fill in your actual values below. For production, use Databricks secrets instead.
# See: config/README.md for complete guide
#
# ═══════════════════════════════════════════════════════════════════════════════

# ─────────────────────────────────────────────────────────────────────────────
# DATABRICKS CONFIGURATION (Required for Databricks features)
# ─────────────────────────────────────────────────────────────────────────────

# Your Databricks workspace URL (e.g., https://company.cloud.databricks.com)
DATABRICKS_WORKSPACE_URL=<YOUR_WORKSPACE_URL>

# Alternative: Just the host (without https://)
# DATABRICKS_HOST=<YOUR_HOST>

# Personal access token (generate at User Settings > Access Tokens)
# See: https://docs.databricks.com/en/dev-tools/auth.html#personal-access-tokens
DATABRICKS_TOKEN=<YOUR_DATABRICKS_TOKEN>

# SQL Warehouse ID (for SQL queries)
# Find at: SQL Warehouses > Click warehouse > Copy Connection Details > Server hostname
DATABRICKS_SQL_WAREHOUSE_ID=<YOUR_SQL_WAREHOUSE_ID>

# Optional: Cluster ID (for notebook/job execution)
# DATABRICKS_CLUSTER_ID=<YOUR_CLUSTER_ID>

# Default Unity Catalog namespace
DATABRICKS_CATALOG=main
DATABRICKS_SCHEMA=default

# ─────────────────────────────────────────────────────────────────────────────
# UNITY CATALOG TEST/DEV CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────

# Test catalog and schema for development
TEST_CATALOG=main
TEST_SCHEMA=default

# Optional: Specific test resources
# TEST_TABLE=<YOUR_TEST_TABLE>
# TEST_VOLUME=<YOUR_TEST_VOLUME>

# ─────────────────────────────────────────────────────────────────────────────
# AI PROVIDER CONFIGURATION (Optional - for AI-assisted scaffolding)
# ─────────────────────────────────────────────────────────────────────────────

# Which AI provider to use: "openai", "claude", "gemini", or "template" (default)
AI_PROVIDER=template

# OpenAI Configuration (if using OpenAI)
# Get your key at: https://platform.openai.com/api-keys
# OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>

# Anthropic Claude Configuration (if using Claude)
# Get your key at: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY=<YOUR_ANTHROPIC_API_KEY>

# Google Gemini Configuration (if using Gemini)
# Get your key at: https://makersuite.google.com/app/apikey
# GOOGLE_API_KEY=<YOUR_GOOGLE_API_KEY>

# ─────────────────────────────────────────────────────────────────────────────
# APPLICATION CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────

# Environment: development, staging, production
ENVIRONMENT=development

# Enable debug mode
DEBUG=true

# ─────────────────────────────────────────────────────────────────────────────
# DATABRICKS SECRETS CONFIGURATION (For production use)
# ─────────────────────────────────────────────────────────────────────────────

# Name of Databricks secret scope (if using secrets instead of .env)
# Leave commented out for local development with .env file
# SECRET_SCOPE=<YOUR_SECRET_SCOPE_NAME>

# ═══════════════════════════════════════════════════════════════════════════════
# REQUIRED VALUES TO FILL IN:
# ═══════════════════════════════════════════════════════════════════════════════
#
# Minimum required for Databricks features:
#   1. DATABRICKS_WORKSPACE_URL=https://your-workspace.cloud.databricks.com
#   2. DATABRICKS_TOKEN=dapi123456789abcdef...
#   3. DATABRICKS_SQL_WAREHOUSE_ID=abc123def456...
#
# Optional but recommended:
#   - TEST_CATALOG=main (or your test catalog)
#   - TEST_SCHEMA=default (or your test schema)
#
# For AI-assisted features (optional):
#   - AI_PROVIDER=openai|claude|gemini
#   - <PROVIDER>_API_KEY=your_key_here
#
# ═══════════════════════════════════════════════════════════════════════════════
#
# HOW TO GET THESE VALUES:
# ═══════════════════════════════════════════════════════════════════════════════
#
# 1. DATABRICKS_WORKSPACE_URL:
#    - Go to your Databricks workspace in browser
#    - Copy the URL (e.g., https://dbc-abc123-def456.cloud.databricks.com)
#
# 2. DATABRICKS_TOKEN:
#    - Click your profile icon (top right)
#    - Settings > Developer > Access Tokens
#    - Click "Generate new token"
#    - Copy the token (starts with "dapi")
#    - Keep it secure! It's like a password.
#
# 3. DATABRICKS_SQL_WAREHOUSE_ID:
#    - Go to SQL Warehouses in Databricks UI
#    - Click on your warehouse
#    - Look for "Server hostname" in Connection Details
#    - The ID is the last part (e.g., abc123def456)
#    - Or use the SQL Warehouse UI and get from URL
#
# 4. TEST_CATALOG and TEST_SCHEMA:
#    - Use "main" and "default" for basic testing
#    - Or create a dedicated test catalog for isolation
#
# 5. AI Provider Keys (if using AI features):
#    - OpenAI: https://platform.openai.com/api-keys
#    - Claude: https://console.anthropic.com/settings/keys
#    - Gemini: https://makersuite.google.com/app/apikey
#
# ═══════════════════════════════════════════════════════════════════════════════
#
# SECURITY NOTES:
# ═══════════════════════════════════════════════════════════════════════════════
#
# ⚠️  NEVER commit this file to git! It's already in .gitignore
# ⚠️  NEVER share your tokens with anyone
# ⚠️  For production, use Databricks secrets instead (see config/README.md)
# ✅  Rotate tokens regularly (every 90 days recommended)
# ✅  Use workspace-scoped tokens when possible
# ✅  For production, use service principals instead of personal tokens
#
# ═══════════════════════════════════════════════════════════════════════════════

# ─────────────────────────────────────────────────────────────────────────────
# ADVANCED DATABRICKS RESOURCES (For integration tests)
# ─────────────────────────────────────────────────────────────────────────────

# Jobs Configuration (for jobs integration tests)
# Create a test job in Databricks UI and put its ID here
# TEST_JOB_ID=<YOUR_TEST_JOB_ID>
# TEST_JOB_NAME=test-lakehouse-appkit-job

# Vector Search Configuration (for vector search integration tests)
# Create a vector search endpoint in Databricks UI
# TEST_VECTOR_ENDPOINT_NAME=<YOUR_VECTOR_ENDPOINT_NAME>
# TEST_VECTOR_INDEX_NAME=<YOUR_VECTOR_INDEX_NAME>
# TEST_VECTOR_SOURCE_TABLE=<YOUR_SOURCE_TABLE_FULL_NAME>  # e.g., main.default.embeddings_table

# Notebooks Configuration (for notebook integration tests)
# Upload a test notebook to your workspace
# TEST_NOTEBOOK_PATH=<YOUR_NOTEBOOK_PATH>  # e.g., /Users/your.email@company.com/test_notebook

# Delta Lake Configuration (for delta operations integration tests)
# Use an existing delta table or specify one to create
# TEST_DELTA_TABLE=<YOUR_DELTA_TABLE>  # e.g., main.default.test_delta_table

# App Deployment Configuration (for app deployment integration tests)
# TEST_APP_NAME=test-lakehouse-app
# TEST_APP_DESCRIPTION=Test app for Lakehouse-AppKit

# Secrets Configuration (for secrets integration tests)
# Create a test secret scope in Databricks
# TEST_SECRET_SCOPE=test-lakehouse-appkit-scope
# TEST_SECRET_KEY=test-api-key

# ─────────────────────────────────────────────────────────────────────────────
# HOW TO CREATE TEST RESOURCES:
# ─────────────────────────────────────────────────────────────────────────────
#
# 1. TEST_JOB_ID:
#    - Go to Workflows in Databricks UI
#    - Click "Create Job"
#    - Configure a simple job (e.g., notebook task)
#    - After creation, click the job and copy the ID from the URL
#
# 2. TEST_VECTOR_ENDPOINT_NAME:
#    - Go to Compute > Vector Search in Databricks UI
#    - Click "Create Endpoint"
#    - Name it (e.g., "test-endpoint")
#    - Wait for it to become ONLINE
#
# 3. TEST_VECTOR_INDEX_NAME:
#    - Go to Vector Search Endpoints
#    - Click your endpoint
#    - Click "Create Index"
#    - Provide a source table with embeddings
#    - Name your index (e.g., "test-index")
#
# 4. TEST_NOTEBOOK_PATH:
#    - Go to Workspace in Databricks UI
#    - Navigate to your user folder
#    - Create a simple Python notebook
#    - Copy the path (e.g., /Users/your.email@company.com/test)
#
# 5. TEST_DELTA_TABLE:
#    - Use SQL editor or notebook:
#      CREATE TABLE main.default.test_delta_table (id INT, value STRING)
#      USING DELTA
#    - Insert some test data
#
# 6. TEST_SECRET_SCOPE:
#    - Use Databricks CLI or UI:
#      databricks secrets create-scope test-lakehouse-appkit-scope
#    - Or use the Lakehouse-AppKit CLI:
#      lakehouse-appkit secrets create-scope test-lakehouse-appkit-scope
#
# ═══════════════════════════════════════════════════════════════════════════════


# =============================================================================
# RESILIENCE PATTERNS (Development Settings)
# =============================================================================
# Configure retry, rate limiting, and circuit breaker patterns
# These are balanced defaults for development

# Retry Configuration
RESILIENCE_MAX_RETRIES=3
RESILIENCE_RETRY_STRATEGY=exponential
RESILIENCE_BASE_DELAY=1.0
RESILIENCE_MAX_DELAY=60.0

# Rate Limiting (moderate for dev)
RESILIENCE_RATE_LIMIT_CALLS=20
RESILIENCE_RATE_LIMIT_WINDOW=1.0

# Circuit Breaker
RESILIENCE_CIRCUIT_BREAKER_ENABLED=true
RESILIENCE_CIRCUIT_BREAKER_THRESHOLD=10
RESILIENCE_CIRCUIT_BREAKER_TIMEOUT=60.0
RESILIENCE_CIRCUIT_BREAKER_HALF_OPEN_CALLS=3

# Request Timeout
RESILIENCE_REQUEST_TIMEOUT=30.0

